{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mount Google Drive\n",
    "\n",
    "# Change directory to your model folder\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/character\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_characters(text):\n",
    "    name_pattern = r\"(\\w+)\\s+(‡§ï‡§π‡§§‡§æ ‡§π‡•à|‡§ï‡§π‡§§‡•Ä ‡§π‡•à|‡§¨‡•ã‡§≤‡§æ|‡§¨‡•ã‡§≤‡•Ä|‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•à|‡§¨‡•ã‡§≤‡§§‡•Ä ‡§π‡•à)\"\n",
    "    characters = set()\n",
    "\n",
    "    for match in re.finditer(name_pattern, text):\n",
    "        characters.add(match.group(1))\n",
    "\n",
    "    # Additional manual patterns for this specific story\n",
    "    for name in [\"‡§∞‡§æ‡§Æ\", \"‡§ï‡§æ‡§≤‡•Ç\", \"‡§∏‡•ã‡§®‡•Ä\"]:\n",
    "        if name in text:\n",
    "            characters.add(name)\n",
    "\n",
    "    return list(characters)\n",
    "\n",
    "def split_scenes(text):\n",
    "    sentences = [s.strip() + \"‡•§\" for s in text.split(\"‡•§\") if s.strip()]\n",
    "    scenes = []\n",
    "    all_characters = extract_characters(text)\n",
    "    current_location = \"‡§Ö‡§ú‡•ç‡§û‡§æ‡§§\"\n",
    "\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        if \"‡§ú‡§Ç‡§ó‡§≤\" in sentence:\n",
    "            current_location = \"‡§ú‡§Ç‡§ó‡§≤\"\n",
    "        elif \"‡§ù‡•Ä‡§≤\" in sentence:\n",
    "            current_location = \"‡§ù‡•Ä‡§≤\"\n",
    "\n",
    "        chars_in_scene = [c for c in all_characters if c in sentence]\n",
    "        if \"‡§§‡•Ä‡§®‡•ã‡§Ç\" in sentence or \"‡§∏‡§≠‡•Ä\" in sentence:\n",
    "            chars_in_scene = all_characters\n",
    "\n",
    "        # Detect dialogue text inside single quotes\n",
    "        dialogues = []\n",
    "        matches = re.findall(r\"'(.*?)'\", sentence)\n",
    "        for d in matches:\n",
    "            dialogues.append(d.strip(\"‡•§\"))\n",
    "\n",
    "        scene_data = {\n",
    "            \"scene_number\": i,\n",
    "            \"location\": current_location,\n",
    "            \"scene_text\": sentence,\n",
    "            \"characters_in_scene\": chars_in_scene,\n",
    "            \"emotion\": detect_scene_emotion(sentence),\n",
    "            \"position\": detect_scene_position(sentence),\n",
    "            \"dialogues\": dialogues\n",
    "        }\n",
    "\n",
    "        scenes.append(scene_data)\n",
    "\n",
    "    return scenes\n",
    "\n",
    "def detect_emotion(text):\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in [\"‡§ñ‡•Å‡§∂\", \"‡§π‡§Å‡§∏\", \"‡§Æ‡•Å‡§∏‡•ç‡§ï‡•Å‡§∞\"]):\n",
    "        return \"happy\"\n",
    "    elif any(word in text for word in [\"‡§°‡§∞\", \"‡§≠‡§Ø\"]):\n",
    "        return \"scared\"\n",
    "    elif any(word in text for word in [\"‡§¶‡•Å‡§ñ\", \"‡§â‡§¶‡§æ‡§∏\"]):\n",
    "        return \"sad\"\n",
    "    return \"smile\"\n",
    "\n",
    "def detect_position(sentence, character):\n",
    "    sentence = sentence.lower()\n",
    "    if \"‡§¨‡•à‡§†\" in sentence:\n",
    "        return \"sitting\"\n",
    "    elif \"‡§≤‡•á‡§ü\" in sentence:\n",
    "        return \"lying\"\n",
    "    elif \"‡§ö‡§≤\" in sentence:\n",
    "        return \"walking\"\n",
    "    return \"standing\"\n",
    "\n",
    "def detect_scene_emotion(sentence):\n",
    "    return detect_emotion(sentence)\n",
    "\n",
    "def detect_scene_position(sentence):\n",
    "    if \"‡§¨‡•à‡§†\" in sentence:\n",
    "        return \"sitting\"\n",
    "    return \"standing\"\n",
    "\n",
    "# Hindi story\n",
    "hindi_story = \"\"\"\n",
    "‡§∞‡§æ‡§Æ, ‡§è‡§ï ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§≤‡§°‡§º‡§ï‡§æ, ‡§ú‡§Ç‡§ó‡§≤ ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§ï‡§æ‡§≤‡•Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à‡•§\n",
    "‡§µ‡§π‡§æ‡§Å ‡§â‡§∏‡§ï‡•Ä ‡§Æ‡•Å‡§≤‡§æ‡§ï‡§æ‡§§ ‡§∏‡•ã‡§®‡•Ä ‡§®‡§æ‡§Æ ‡§ï‡•Ä ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡•Ä ‡§∏‡•á ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§ï‡§æ‡§≤‡•Ç ‡§ï‡§π‡§§‡§æ ‡§π‡•à '‡§®‡§Æ‡§∏‡•ç‡§§‡•á'‡•§ ‡§∏‡•ã‡§®‡•Ä ‡§ï‡§π‡§§‡•Ä ‡§π‡•à '‡§π‡§æ‡§Ø'‡•§\n",
    "‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§§‡•Ä‡§®‡•ã‡§Ç ‡§è‡§ï ‡§ù‡•Ä‡§≤ ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§¨‡•à‡§†‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§æ‡§§‡•á‡§Ç ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§\n",
    "\"\"\"\n",
    "\n",
    "characters = extract_characters(hindi_story)\n",
    "scenes = split_scenes(hindi_story)\n",
    "\n",
    "story_data = {\n",
    "    \"original_hindi\": hindi_story.strip(),\n",
    "    \"characters\": characters,\n",
    "    \"scenes\": scenes\n",
    "}\n",
    "\n",
    "with open(\"hindi_story.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(story_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Hindi story processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate bitsandbytes xformers safetensors\n",
    "!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import functional as F\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ‚úÖ Set Random Seeds for Reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ‚úÖ Configuration\n",
    "class Config:\n",
    "    model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "    dataset_dir = \"/content/drive/MyDrive/character/train\"\n",
    "    output_dir = \"/content/drive/MyDrive/lora_trained_model\"\n",
    "    batch_size = 1\n",
    "    gradient_accumulation_steps = 4\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 10\n",
    "    mixed_precision = \"fp16\"\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    target_modules = [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]\n",
    "    validation_steps = 100\n",
    "    characters = sorted(os.listdir(dataset_dir))  # Sort characters for consistent order\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ‚úÖ Load Stable Diffusion Model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    config.model_id, torch_dtype=torch.float16 if config.mixed_precision == \"fp16\" else torch.float32\n",
    ")\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# ‚úÖ Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_dropout=config.lora_dropout\n",
    ")\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
    "pipe.unet.train()\n",
    "\n",
    "# ‚úÖ Define Data Augmentation & Custom Image Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, character, transform=None):\n",
    "        self.root_dir = os.path.join(root_dir, character)\n",
    "        self.character = character\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for emotion in sorted(os.listdir(self.root_dir)):\n",
    "            emotion_path = os.path.join(self.root_dir, emotion)\n",
    "            if not os.path.isdir(emotion_path):  # ‚úÖ Skip files\n",
    "                continue\n",
    "\n",
    "            emotion_path = os.path.join(self.root_dir, emotion)\n",
    "            if not os.path.isdir(emotion_path):\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(emotion_path):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(emotion_path, file))\n",
    "                    self.labels.append(f\"{character}_{emotion}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        prompt = f\"A {label.split('_')[1]} {label.split('_')[0]} in comic style\"\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\"pixel_values\": image, \"prompt\": prompt}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# ‚úÖ Training Loop for Each Character\n",
    "for character in config.characters:\n",
    "    print(f\"\\nüöÄ Training for character: {character}\")\n",
    "    char_dataset = ImageDataset(config.dataset_dir, character, transform=transform)\n",
    "    char_dataloader = DataLoader(char_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(char_dataloader) * config.num_epochs // config.gradient_accumulation_steps)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(char_dataloader), desc=f\"{character} - Epoch {epoch+1}\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(char_dataloader):\n",
    "            pixel_values = batch[\"pixel_values\"].to(\"cuda\").half()\n",
    "            latents = pipe.vae.encode(pixel_values).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=\"cuda\").long()\n",
    "            noise = torch.randn_like(latents)\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "            text_input = pipe.tokenizer(batch[\"prompt\"], padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "            encoder_hidden_states = pipe.text_encoder(**text_input).last_hidden_state\n",
    "            model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred, noise) / config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        progress_bar.close()\n",
    "\n",
    "    # ‚úÖ Save Checkpoint for Each Character\n",
    "    char_save_dir = os.path.join(config.output_dir, f\"{character}_lora\")\n",
    "    os.makedirs(char_save_dir, exist_ok=True)\n",
    "    lora_save_dir = os.path.join(char_save_dir, \"lora_adapter\")\n",
    "    pipe.unet.save_pretrained(lora_save_dir)\n",
    "    print(f\"‚úÖ LoRA model saved for {character} at {char_save_dir}\")\n",
    "\n",
    "print(\"üéâ Training Completed for All Characters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from googletrans import Translator\n",
    "\n",
    "def translate_story(input_file, output_file):\n",
    "    # Initialize translator\n",
    "    translator = Translator()\n",
    "\n",
    "    # Load Hindi story\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        hindi_data = json.load(f)\n",
    "\n",
    "    # Translate the main story text\n",
    "    translated_story = translator.translate(hindi_data['original_hindi'], src='hi', dest='en').text\n",
    "\n",
    "    # Prepare English data structure\n",
    "    english_data = {\n",
    "        \"original_english\": translated_story,\n",
    "        \"characters\": [],\n",
    "        \"scenes\": []\n",
    "    }\n",
    "\n",
    "    # Translate character names (we'll keep these as is for LoRA mapping)\n",
    "    english_data['characters'] = hindi_data['characters']\n",
    "\n",
    "    # Translate each scene\n",
    "    for scene in hindi_data['scenes']:\n",
    "        translated_scene = {\n",
    "            \"scene_number\": scene[\"scene_number\"],\n",
    "            \"location\": translator.translate(scene[\"location\"], src='hi', dest='en').text,\n",
    "            \"scene_text\": translator.translate(scene[\"scene_text\"], src='hi', dest='en').text,\n",
    "            \"characters_in_scene\": scene[\"characters_in_scene\"],  # Keep original for mapping\n",
    "            \"emotion\": scene[\"emotion\"],  # Already in English\n",
    "            \"position\": scene[\"position\"],  # Already in English\n",
    "            \"dialogues\": [translator.translate(d, src='hi', dest='en').text for d in scene[\"dialogues\"]]\n",
    "        }\n",
    "        english_data['scenes'].append(translated_scene)\n",
    "\n",
    "    # Save translated story\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(english_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Successfully translated {input_file} to {output_file}\")\n",
    "\n",
    "# Usage\n",
    "translate_story(\"hindi_story.json\", \"translated_story.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Your Hindi to English character mapping\n",
    "character_map = {\n",
    "    \"‡§ï‡§æ‡§≤‡•Ç\": \"Bruno\",\n",
    "    \"‡§∏‡•ã‡§®‡•Ä\": \"Tina\",\n",
    "    \"‡§∞‡§æ‡§Æ\": \"Ram\"\n",
    "}\n",
    "\n",
    "# Optional: LoRA adapter mapping (if needed later)\n",
    "lora_model_map = {\n",
    "    \"Kalu\": \"Bruno\",\n",
    "    \"Soni\": \"Tina\",\n",
    "    \"Ram\": \"Ram\"\n",
    "}\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"translated_story.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Update character names in each scene\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene[\"characters_in_scene\"] = [character_map.get(c, c) for c in scene[\"characters_in_scene\"]]\n",
    "\n",
    "# Replace characters list with English-mapped ones (if needed)\n",
    "data[\"characters\"] = [character_map.get(c, c) for c in data[\"characters\"]]\n",
    "\n",
    "# Save the updated file\n",
    "with open(\"english.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Character mapping done and saved to 'english.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# ‚úÖ Configuration\n",
    "class Config:\n",
    "    base_model = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "    lora_weights_path = \"/content/drive/MyDrive/lora_trained_model\"\n",
    "    output_dir = \"./panels/\"\n",
    "    json_file = \"english.json\"\n",
    "\n",
    "# ‚úÖ Create output folder\n",
    "os.makedirs(Config.output_dir, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Style prompts based on known visual references\n",
    "character_prompts = {\n",
    "    \"Ram\": \"a charming young man with a blonde hair and blue eyes, inspired by Flynn Rider from Tangled, Disney-style\",\n",
    "    \"Bruno\": \"a goofy, friendly Great Dane dog, specifically a cartoon dog like Scooby-Doo\",\n",
    "    \"Tina\": \"a brave island girl with long curly black hair and tropical attire, inspired by Moana, Disney-style\"\n",
    "}\n",
    "\n",
    "# ‚úÖ Cache for reusing loaded models\n",
    "pipe_cache = {}\n",
    "\n",
    "def load_model_for_inference(character):\n",
    "    \"\"\"Load and cache model for a specific character using LoRA.\"\"\"\n",
    "    if character in pipe_cache:\n",
    "        return pipe_cache[character]\n",
    "\n",
    "    print(f\"üß† Loading model for {character}\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        Config.base_model, torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    lora_path = os.path.join(Config.lora_weights_path, f\"{character}_lora\", \"lora_adapter\")\n",
    "    if os.path.exists(lora_path):\n",
    "        print(f\"üîó Applying LoRA for {character} from {lora_path}\")\n",
    "        pipe.unet = PeftModel.from_pretrained(pipe.unet, lora_path).merge_and_unload()\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No LoRA found for {character}, using base model.\")\n",
    "\n",
    "    pipe_cache[character] = pipe\n",
    "    return pipe\n",
    "\n",
    "def generate_comic_from_json(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    last_location = None\n",
    "\n",
    "    for scene in data[\"scenes\"]:\n",
    "        scene_number = scene[\"scene_number\"]\n",
    "        scene_text = scene[\"scene_text\"]\n",
    "        location = scene.get(\"location\") or last_location or \"Unknown Place\"\n",
    "        last_location = location\n",
    "\n",
    "        # Determine main character for the scene\n",
    "        character = (\n",
    "            scene[\"characters_in_scene\"][0]\n",
    "            if scene[\"characters_in_scene\"]\n",
    "            else data[\"characters\"][0]\n",
    "        )\n",
    "\n",
    "        # Add location context if needed\n",
    "        if location.lower() not in scene_text.lower():\n",
    "            scene_text = f\"In the {location}, {scene_text}\"\n",
    "\n",
    "        # Add style description\n",
    "        style_desc = character_prompts.get(character, \"\")\n",
    "        prompt = (\n",
    "            f\"{scene_text}. {style_desc}, highly detailed comic-style illustration, vibrant colors, \"\n",
    "            \"inked outlines, dynamic composition, expressive characters, graphic novel style.\"\n",
    "        )\n",
    "\n",
    "        # Load LoRA model\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            pipe = load_model_for_inference(character)\n",
    "\n",
    "            print(f\"üé® Scene {scene_number}: Prompt -> {prompt}\")\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                image = pipe(prompt).images[0]\n",
    "\n",
    "            output_path = os.path.join(Config.output_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "            image.save(output_path)\n",
    "            print(f\"‚úÖ Saved panel at {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating scene {scene_number}: {e}\")\n",
    "\n",
    "    print(\"‚úÖ‚úÖ Comic generation complete!\")\n",
    "\n",
    "# ‚úÖ Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    generate_comic_from_json(Config.json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Noto Sans Devanagari\n",
    "!apt-get install -y fonts-noto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ‚úÖ Configuration\n",
    "class Config:\n",
    "    json_path = \"hindi_story.json\"\n",
    "    panels_dir = \"/content/panels\"\n",
    "    font_path = \"/usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf\"\n",
    "    font_size = 36\n",
    "    output_dir_with_dialogue = \"/content/panels_with_dialogue\"\n",
    "    output_dir_without_dialogue = \"/content/panels_without_dialogue\"\n",
    "\n",
    "# ‚úÖ Create output directories\n",
    "os.makedirs(Config.output_dir_with_dialogue, exist_ok=True)\n",
    "os.makedirs(Config.output_dir_without_dialogue, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(Config.font_path, Config.font_size)\n",
    "except OSError:\n",
    "    raise RuntimeError(f\"Could not load font from: {Config.font_path}\")\n",
    "\n",
    "# ‚úÖ Load story JSON\n",
    "with open(Config.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ‚úÖ Process each scene\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene_number = scene[\"scene_number\"]\n",
    "    dialogues = scene.get(\"dialogues\", [])\n",
    "\n",
    "    image_path = os.path.join(Config.panels_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"‚ö†Ô∏è Scene {scene_number}: No matching image found.\")\n",
    "        continue\n",
    "\n",
    "    # ‚úÖ Save the original image as \"without dialogue\"\n",
    "    image = Image.open(image_path)\n",
    "    no_dialogue_path = os.path.join(Config.output_dir_without_dialogue, f\"comic_panel_{scene_number}.bmp\")\n",
    "    image.save(no_dialogue_path)\n",
    "    print(f\"üìÅ Scene {scene_number}: Saved without dialogue to -> {no_dialogue_path}\")\n",
    "\n",
    "    # ‚úÖ If no dialogues, skip creating a second version\n",
    "    if not dialogues:\n",
    "        continue\n",
    "\n",
    "    # ‚úÖ Create copy for dialogue version\n",
    "    image_with_dialogue = image.copy()\n",
    "    draw = ImageDraw.Draw(image_with_dialogue)\n",
    "\n",
    "    x, y = 50, 50\n",
    "    line_height = Config.font_size + 30\n",
    "    padding = 15\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        bbox = draw.textbbox((x, y), dialogue, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "\n",
    "        # White bubble with black outline\n",
    "        box_coords = [x - padding, y - padding, x + text_width + padding, y + text_height + padding]\n",
    "        draw.rectangle(box_coords, fill=\"white\", outline=\"black\", width=2)\n",
    "\n",
    "        draw.text((x, y), dialogue, font=font, fill=\"black\")\n",
    "        y += line_height\n",
    "\n",
    "    # ‚úÖ Save dialogue image\n",
    "    dialogue_path = os.path.join(Config.output_dir_with_dialogue, f\"comic_panel_{scene_number}.bmp\")\n",
    "    image_with_dialogue.save(dialogue_path)\n",
    "    print(f\"üí¨ Scene {scene_number}: Dialogue version saved to -> {dialogue_path}\")\n",
    "\n",
    "print(\"‚úÖ‚úÖ Done! All panels saved with and without dialogues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ‚úÖ Configuration\n",
    "class Config:\n",
    "    json_path = \"hindi_story.json\"\n",
    "    dialogue_dir = \"/content/panels_with_dialogue\"\n",
    "    no_dialogue_dir = \"/content/panels_without_dialogue\"\n",
    "    final_strip_path = \"/content/final_comic_strip.bmp\"\n",
    "    layout = \"horizontal\"  # or \"vertical\"\n",
    "\n",
    "# ‚úÖ Load JSON\n",
    "with open(Config.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ‚úÖ Gather all scene images in order\n",
    "scene_images = []\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene_number = scene[\"scene_number\"]\n",
    "    has_dialogue = bool(scene.get(\"dialogues\"))\n",
    "\n",
    "    # Pick correct image based on presence of dialogues\n",
    "    if has_dialogue:\n",
    "        img_path = os.path.join(Config.dialogue_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "    else:\n",
    "        img_path = os.path.join(Config.no_dialogue_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"‚ö†Ô∏è Image for scene {scene_number} not found at {img_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "    scene_images.append(image)\n",
    "\n",
    "# ‚úÖ Combine images\n",
    "if not scene_images:\n",
    "    raise RuntimeError(\"No images found to create comic strip.\")\n",
    "\n",
    "# Dimensions for final canvas\n",
    "if Config.layout == \"horizontal\":\n",
    "    total_width = sum(img.width for img in scene_images)\n",
    "    max_height = max(img.height for img in scene_images)\n",
    "    final_image = Image.new(\"RGB\", (total_width, max_height), color=(255, 255, 255))\n",
    "\n",
    "    x_offset = 0\n",
    "    for img in scene_images:\n",
    "        final_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "\n",
    "else:  # vertical layout\n",
    "    max_width = max(img.width for img in scene_images)\n",
    "    total_height = sum(img.height for img in scene_images)\n",
    "    final_image = Image.new(\"RGB\", (max_width, total_height), color=(255, 255, 255))\n",
    "\n",
    "    y_offset = 0\n",
    "    for img in scene_images:\n",
    "        final_image.paste(img, (0, y_offset))\n",
    "        y_offset += img.height\n",
    "\n",
    "# ‚úÖ Save the final comic strip\n",
    "final_image.save(Config.final_strip_path)\n",
    "print(f\"üéâ Comic strip created: {Config.final_strip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
