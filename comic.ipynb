{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mount Google Drive\n",
    "\n",
    "# Change directory to your model folder\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/character\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_characters(text):\n",
    "    name_pattern = r\"(\\w+)\\s+(कहता है|कहती है|बोला|बोली|बोलता है|बोलती है)\"\n",
    "    characters = set()\n",
    "\n",
    "    for match in re.finditer(name_pattern, text):\n",
    "        characters.add(match.group(1))\n",
    "\n",
    "    # Additional manual patterns for this specific story\n",
    "    for name in [\"राम\", \"कालू\", \"सोनी\"]:\n",
    "        if name in text:\n",
    "            characters.add(name)\n",
    "\n",
    "    return list(characters)\n",
    "\n",
    "def split_scenes(text):\n",
    "    sentences = [s.strip() + \"।\" for s in text.split(\"।\") if s.strip()]\n",
    "    scenes = []\n",
    "    all_characters = extract_characters(text)\n",
    "    current_location = \"अज्ञात\"\n",
    "\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        if \"जंगल\" in sentence:\n",
    "            current_location = \"जंगल\"\n",
    "        elif \"झील\" in sentence:\n",
    "            current_location = \"झील\"\n",
    "\n",
    "        chars_in_scene = [c for c in all_characters if c in sentence]\n",
    "        if \"तीनों\" in sentence or \"सभी\" in sentence:\n",
    "            chars_in_scene = all_characters\n",
    "\n",
    "        # Detect dialogue text inside single quotes\n",
    "        dialogues = []\n",
    "        matches = re.findall(r\"'(.*?)'\", sentence)\n",
    "        for d in matches:\n",
    "            dialogues.append(d.strip(\"।\"))\n",
    "\n",
    "        scene_data = {\n",
    "            \"scene_number\": i,\n",
    "            \"location\": current_location,\n",
    "            \"scene_text\": sentence,\n",
    "            \"characters_in_scene\": chars_in_scene,\n",
    "            \"emotion\": detect_scene_emotion(sentence),\n",
    "            \"position\": detect_scene_position(sentence),\n",
    "            \"dialogues\": dialogues\n",
    "        }\n",
    "\n",
    "        scenes.append(scene_data)\n",
    "\n",
    "    return scenes\n",
    "\n",
    "def detect_emotion(text):\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in [\"खुश\", \"हँस\", \"मुस्कुर\"]):\n",
    "        return \"happy\"\n",
    "    elif any(word in text for word in [\"डर\", \"भय\"]):\n",
    "        return \"scared\"\n",
    "    elif any(word in text for word in [\"दुख\", \"उदास\"]):\n",
    "        return \"sad\"\n",
    "    return \"smile\"\n",
    "\n",
    "def detect_position(sentence, character):\n",
    "    sentence = sentence.lower()\n",
    "    if \"बैठ\" in sentence:\n",
    "        return \"sitting\"\n",
    "    elif \"लेट\" in sentence:\n",
    "        return \"lying\"\n",
    "    elif \"चल\" in sentence:\n",
    "        return \"walking\"\n",
    "    return \"standing\"\n",
    "\n",
    "def detect_scene_emotion(sentence):\n",
    "    return detect_emotion(sentence)\n",
    "\n",
    "def detect_scene_position(sentence):\n",
    "    if \"बैठ\" in sentence:\n",
    "        return \"sitting\"\n",
    "    return \"standing\"\n",
    "\n",
    "# Hindi story\n",
    "hindi_story = \"\"\"\n",
    "राम, एक सुंदर लड़का, जंगल में अपने बात करने वाले कुत्ते कालू के साथ चल रहा है।\n",
    "वहाँ उसकी मुलाकात सोनी नाम की एक लड़की से होती है। कालू कहता है 'नमस्ते'। सोनी कहती है 'हाय'।\n",
    "बाद में तीनों एक झील के पास बैठते हैं और बातें करते हैं।\n",
    "\"\"\"\n",
    "\n",
    "characters = extract_characters(hindi_story)\n",
    "scenes = split_scenes(hindi_story)\n",
    "\n",
    "story_data = {\n",
    "    \"original_hindi\": hindi_story.strip(),\n",
    "    \"characters\": characters,\n",
    "    \"scenes\": scenes\n",
    "}\n",
    "\n",
    "with open(\"hindi_story.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(story_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Hindi story processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate bitsandbytes xformers safetensors\n",
    "!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import functional as F\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ✅ Set Random Seeds for Reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ✅ Configuration\n",
    "class Config:\n",
    "    model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "    dataset_dir = \"/content/drive/MyDrive/character/train\"\n",
    "    output_dir = \"/content/drive/MyDrive/lora_trained_model\"\n",
    "    batch_size = 1\n",
    "    gradient_accumulation_steps = 4\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 10\n",
    "    mixed_precision = \"fp16\"\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    target_modules = [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]\n",
    "    validation_steps = 100\n",
    "    characters = sorted(os.listdir(dataset_dir))  # Sort characters for consistent order\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ✅ Load Stable Diffusion Model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    config.model_id, torch_dtype=torch.float16 if config.mixed_precision == \"fp16\" else torch.float32\n",
    ")\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# ✅ Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_dropout=config.lora_dropout\n",
    ")\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
    "pipe.unet.train()\n",
    "\n",
    "# ✅ Define Data Augmentation & Custom Image Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, character, transform=None):\n",
    "        self.root_dir = os.path.join(root_dir, character)\n",
    "        self.character = character\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for emotion in sorted(os.listdir(self.root_dir)):\n",
    "            emotion_path = os.path.join(self.root_dir, emotion)\n",
    "            if not os.path.isdir(emotion_path):  # ✅ Skip files\n",
    "                continue\n",
    "\n",
    "            emotion_path = os.path.join(self.root_dir, emotion)\n",
    "            if not os.path.isdir(emotion_path):\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(emotion_path):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(emotion_path, file))\n",
    "                    self.labels.append(f\"{character}_{emotion}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        prompt = f\"A {label.split('_')[1]} {label.split('_')[0]} in comic style\"\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {\"pixel_values\": image, \"prompt\": prompt}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# ✅ Training Loop for Each Character\n",
    "for character in config.characters:\n",
    "    print(f\"\\n🚀 Training for character: {character}\")\n",
    "    char_dataset = ImageDataset(config.dataset_dir, character, transform=transform)\n",
    "    char_dataloader = DataLoader(char_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(char_dataloader) * config.num_epochs // config.gradient_accumulation_steps)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(char_dataloader), desc=f\"{character} - Epoch {epoch+1}\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(char_dataloader):\n",
    "            pixel_values = batch[\"pixel_values\"].to(\"cuda\").half()\n",
    "            latents = pipe.vae.encode(pixel_values).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=\"cuda\").long()\n",
    "            noise = torch.randn_like(latents)\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "            text_input = pipe.tokenizer(batch[\"prompt\"], padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "            encoder_hidden_states = pipe.text_encoder(**text_input).last_hidden_state\n",
    "            model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred, noise) / config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        progress_bar.close()\n",
    "\n",
    "    # ✅ Save Checkpoint for Each Character\n",
    "    char_save_dir = os.path.join(config.output_dir, f\"{character}_lora\")\n",
    "    os.makedirs(char_save_dir, exist_ok=True)\n",
    "    lora_save_dir = os.path.join(char_save_dir, \"lora_adapter\")\n",
    "    pipe.unet.save_pretrained(lora_save_dir)\n",
    "    print(f\"✅ LoRA model saved for {character} at {char_save_dir}\")\n",
    "\n",
    "print(\"🎉 Training Completed for All Characters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from googletrans import Translator\n",
    "\n",
    "def translate_story(input_file, output_file):\n",
    "    # Initialize translator\n",
    "    translator = Translator()\n",
    "\n",
    "    # Load Hindi story\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        hindi_data = json.load(f)\n",
    "\n",
    "    # Translate the main story text\n",
    "    translated_story = translator.translate(hindi_data['original_hindi'], src='hi', dest='en').text\n",
    "\n",
    "    # Prepare English data structure\n",
    "    english_data = {\n",
    "        \"original_english\": translated_story,\n",
    "        \"characters\": [],\n",
    "        \"scenes\": []\n",
    "    }\n",
    "\n",
    "    # Translate character names (we'll keep these as is for LoRA mapping)\n",
    "    english_data['characters'] = hindi_data['characters']\n",
    "\n",
    "    # Translate each scene\n",
    "    for scene in hindi_data['scenes']:\n",
    "        translated_scene = {\n",
    "            \"scene_number\": scene[\"scene_number\"],\n",
    "            \"location\": translator.translate(scene[\"location\"], src='hi', dest='en').text,\n",
    "            \"scene_text\": translator.translate(scene[\"scene_text\"], src='hi', dest='en').text,\n",
    "            \"characters_in_scene\": scene[\"characters_in_scene\"],  # Keep original for mapping\n",
    "            \"emotion\": scene[\"emotion\"],  # Already in English\n",
    "            \"position\": scene[\"position\"],  # Already in English\n",
    "            \"dialogues\": [translator.translate(d, src='hi', dest='en').text for d in scene[\"dialogues\"]]\n",
    "        }\n",
    "        english_data['scenes'].append(translated_scene)\n",
    "\n",
    "    # Save translated story\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(english_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"✅ Successfully translated {input_file} to {output_file}\")\n",
    "\n",
    "# Usage\n",
    "translate_story(\"hindi_story.json\", \"translated_story.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Your Hindi to English character mapping\n",
    "character_map = {\n",
    "    \"कालू\": \"Bruno\",\n",
    "    \"सोनी\": \"Tina\",\n",
    "    \"राम\": \"Ram\"\n",
    "}\n",
    "\n",
    "# Optional: LoRA adapter mapping (if needed later)\n",
    "lora_model_map = {\n",
    "    \"Kalu\": \"Bruno\",\n",
    "    \"Soni\": \"Tina\",\n",
    "    \"Ram\": \"Ram\"\n",
    "}\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"translated_story.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Update character names in each scene\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene[\"characters_in_scene\"] = [character_map.get(c, c) for c in scene[\"characters_in_scene\"]]\n",
    "\n",
    "# Replace characters list with English-mapped ones (if needed)\n",
    "data[\"characters\"] = [character_map.get(c, c) for c in data[\"characters\"]]\n",
    "\n",
    "# Save the updated file\n",
    "with open(\"english.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Character mapping done and saved to 'english.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# ✅ Configuration\n",
    "class Config:\n",
    "    base_model = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "    lora_weights_path = \"/content/drive/MyDrive/lora_trained_model\"\n",
    "    output_dir = \"./panels/\"\n",
    "    json_file = \"english.json\"\n",
    "\n",
    "# ✅ Create output folder\n",
    "os.makedirs(Config.output_dir, exist_ok=True)\n",
    "\n",
    "# ✅ Style prompts based on known visual references\n",
    "character_prompts = {\n",
    "    \"Ram\": \"a charming young man with a blonde hair and blue eyes, inspired by Flynn Rider from Tangled, Disney-style\",\n",
    "    \"Bruno\": \"a goofy, friendly Great Dane dog, specifically a cartoon dog like Scooby-Doo\",\n",
    "    \"Tina\": \"a brave island girl with long curly black hair and tropical attire, inspired by Moana, Disney-style\"\n",
    "}\n",
    "\n",
    "# ✅ Cache for reusing loaded models\n",
    "pipe_cache = {}\n",
    "\n",
    "def load_model_for_inference(character):\n",
    "    \"\"\"Load and cache model for a specific character using LoRA.\"\"\"\n",
    "    if character in pipe_cache:\n",
    "        return pipe_cache[character]\n",
    "\n",
    "    print(f\"🧠 Loading model for {character}\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        Config.base_model, torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    lora_path = os.path.join(Config.lora_weights_path, f\"{character}_lora\", \"lora_adapter\")\n",
    "    if os.path.exists(lora_path):\n",
    "        print(f\"🔗 Applying LoRA for {character} from {lora_path}\")\n",
    "        pipe.unet = PeftModel.from_pretrained(pipe.unet, lora_path).merge_and_unload()\n",
    "    else:\n",
    "        print(f\"⚠️ No LoRA found for {character}, using base model.\")\n",
    "\n",
    "    pipe_cache[character] = pipe\n",
    "    return pipe\n",
    "\n",
    "def generate_comic_from_json(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    last_location = None\n",
    "\n",
    "    for scene in data[\"scenes\"]:\n",
    "        scene_number = scene[\"scene_number\"]\n",
    "        scene_text = scene[\"scene_text\"]\n",
    "        location = scene.get(\"location\") or last_location or \"Unknown Place\"\n",
    "        last_location = location\n",
    "\n",
    "        # Determine main character for the scene\n",
    "        character = (\n",
    "            scene[\"characters_in_scene\"][0]\n",
    "            if scene[\"characters_in_scene\"]\n",
    "            else data[\"characters\"][0]\n",
    "        )\n",
    "\n",
    "        # Add location context if needed\n",
    "        if location.lower() not in scene_text.lower():\n",
    "            scene_text = f\"In the {location}, {scene_text}\"\n",
    "\n",
    "        # Add style description\n",
    "        style_desc = character_prompts.get(character, \"\")\n",
    "        prompt = (\n",
    "            f\"{scene_text}. {style_desc}, highly detailed comic-style illustration, vibrant colors, \"\n",
    "            \"inked outlines, dynamic composition, expressive characters, graphic novel style.\"\n",
    "        )\n",
    "\n",
    "        # Load LoRA model\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            pipe = load_model_for_inference(character)\n",
    "\n",
    "            print(f\"🎨 Scene {scene_number}: Prompt -> {prompt}\")\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                image = pipe(prompt).images[0]\n",
    "\n",
    "            output_path = os.path.join(Config.output_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "            image.save(output_path)\n",
    "            print(f\"✅ Saved panel at {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating scene {scene_number}: {e}\")\n",
    "\n",
    "    print(\"✅✅ Comic generation complete!\")\n",
    "\n",
    "# ✅ Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    generate_comic_from_json(Config.json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Noto Sans Devanagari\n",
    "!apt-get install -y fonts-noto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ✅ Configuration\n",
    "class Config:\n",
    "    json_path = \"hindi_story.json\"\n",
    "    panels_dir = \"/content/panels\"\n",
    "    font_path = \"/usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf\"\n",
    "    font_size = 36\n",
    "    output_dir_with_dialogue = \"/content/panels_with_dialogue\"\n",
    "    output_dir_without_dialogue = \"/content/panels_without_dialogue\"\n",
    "\n",
    "# ✅ Create output directories\n",
    "os.makedirs(Config.output_dir_with_dialogue, exist_ok=True)\n",
    "os.makedirs(Config.output_dir_without_dialogue, exist_ok=True)\n",
    "\n",
    "# ✅ Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(Config.font_path, Config.font_size)\n",
    "except OSError:\n",
    "    raise RuntimeError(f\"Could not load font from: {Config.font_path}\")\n",
    "\n",
    "# ✅ Load story JSON\n",
    "with open(Config.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ✅ Process each scene\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene_number = scene[\"scene_number\"]\n",
    "    dialogues = scene.get(\"dialogues\", [])\n",
    "\n",
    "    image_path = os.path.join(Config.panels_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"⚠️ Scene {scene_number}: No matching image found.\")\n",
    "        continue\n",
    "\n",
    "    # ✅ Save the original image as \"without dialogue\"\n",
    "    image = Image.open(image_path)\n",
    "    no_dialogue_path = os.path.join(Config.output_dir_without_dialogue, f\"comic_panel_{scene_number}.bmp\")\n",
    "    image.save(no_dialogue_path)\n",
    "    print(f\"📁 Scene {scene_number}: Saved without dialogue to -> {no_dialogue_path}\")\n",
    "\n",
    "    # ✅ If no dialogues, skip creating a second version\n",
    "    if not dialogues:\n",
    "        continue\n",
    "\n",
    "    # ✅ Create copy for dialogue version\n",
    "    image_with_dialogue = image.copy()\n",
    "    draw = ImageDraw.Draw(image_with_dialogue)\n",
    "\n",
    "    x, y = 50, 50\n",
    "    line_height = Config.font_size + 30\n",
    "    padding = 15\n",
    "\n",
    "    for dialogue in dialogues:\n",
    "        bbox = draw.textbbox((x, y), dialogue, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "\n",
    "        # White bubble with black outline\n",
    "        box_coords = [x - padding, y - padding, x + text_width + padding, y + text_height + padding]\n",
    "        draw.rectangle(box_coords, fill=\"white\", outline=\"black\", width=2)\n",
    "\n",
    "        draw.text((x, y), dialogue, font=font, fill=\"black\")\n",
    "        y += line_height\n",
    "\n",
    "    # ✅ Save dialogue image\n",
    "    dialogue_path = os.path.join(Config.output_dir_with_dialogue, f\"comic_panel_{scene_number}.bmp\")\n",
    "    image_with_dialogue.save(dialogue_path)\n",
    "    print(f\"💬 Scene {scene_number}: Dialogue version saved to -> {dialogue_path}\")\n",
    "\n",
    "print(\"✅✅ Done! All panels saved with and without dialogues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ✅ Configuration\n",
    "class Config:\n",
    "    json_path = \"hindi_story.json\"\n",
    "    dialogue_dir = \"/content/panels_with_dialogue\"\n",
    "    no_dialogue_dir = \"/content/panels_without_dialogue\"\n",
    "    final_strip_path = \"/content/final_comic_strip.bmp\"\n",
    "    layout = \"horizontal\"  # or \"vertical\"\n",
    "\n",
    "# ✅ Load JSON\n",
    "with open(Config.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ✅ Gather all scene images in order\n",
    "scene_images = []\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene_number = scene[\"scene_number\"]\n",
    "    has_dialogue = bool(scene.get(\"dialogues\"))\n",
    "\n",
    "    # Pick correct image based on presence of dialogues\n",
    "    if has_dialogue:\n",
    "        img_path = os.path.join(Config.dialogue_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "    else:\n",
    "        img_path = os.path.join(Config.no_dialogue_dir, f\"comic_panel_{scene_number}.bmp\")\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"⚠️ Image for scene {scene_number} not found at {img_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "    scene_images.append(image)\n",
    "\n",
    "# ✅ Combine images\n",
    "if not scene_images:\n",
    "    raise RuntimeError(\"No images found to create comic strip.\")\n",
    "\n",
    "# Dimensions for final canvas\n",
    "if Config.layout == \"horizontal\":\n",
    "    total_width = sum(img.width for img in scene_images)\n",
    "    max_height = max(img.height for img in scene_images)\n",
    "    final_image = Image.new(\"RGB\", (total_width, max_height), color=(255, 255, 255))\n",
    "\n",
    "    x_offset = 0\n",
    "    for img in scene_images:\n",
    "        final_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "\n",
    "else:  # vertical layout\n",
    "    max_width = max(img.width for img in scene_images)\n",
    "    total_height = sum(img.height for img in scene_images)\n",
    "    final_image = Image.new(\"RGB\", (max_width, total_height), color=(255, 255, 255))\n",
    "\n",
    "    y_offset = 0\n",
    "    for img in scene_images:\n",
    "        final_image.paste(img, (0, y_offset))\n",
    "        y_offset += img.height\n",
    "\n",
    "# ✅ Save the final comic strip\n",
    "final_image.save(Config.final_strip_path)\n",
    "print(f\"🎉 Comic strip created: {Config.final_strip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
